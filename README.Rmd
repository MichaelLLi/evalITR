---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-"
  )
```


```{r pic1, echo=FALSE, out.width = '100%'}
knitr::include_graphics("man/figures/README-manual.png")
```

# evalITR

<!-- badges: start -->
<!-- badges: end -->

## Installation

You can install the development version of evalITR from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("MichaelLLi/evalITR", ref = "causal-ml")
```


(Optional) if you have multiple cores, we recommendate using multisession futures and processing in parallel. This would increase computation efficiency and reduce the time to fit the model. 

```r
library(furrr)
library(future.apply)

nworkers <- 4
plan(multisession, workers =nworkers)
```



## Example under sample splitting


This is an example using the `star` dataset (for more information about the dataset, please use `?star`). 

We start with a simple example with one outcome variable (writing scores) and one machine learning algorithm (causal forest). Then we move to incoporate multiple outcomes and compare model performances with several machine learning algorithms. 


To begin, we load the dataset and specify the outcome variable and covariates to be used in the model. Next, we utilize a random forest algorithm to develop an Individualized Treatment Rule (ITR) for estimating the varied impacts of small class sizes on students' writing scores.  Since the treatment is often costly for most policy programs, we consider a case with 20% budget constraint (`budget` = 0.2). The model will identify the top 20% of units who benefit from the treatment most and assign them to with the treatment. We train the model through sample splitting, with the `split_ratio` between the train and test sets determined by the `split_ratio` argument. Specifically, we allocate 70% of the data to train the model, while the remaining 30% is used as testing data (`split_ratio` = 0.7).


```{r estimate, message = FALSE}
library(tidyverse)
library(evalITR)

load("data/star.rda")

# specifying the outcome
outcomes <- "g3tlangss"

# specifying the treatment
treatment <- "treatment"

# specifying the data (remove other outcomes)
star_data <- star %>% dplyr::select(-c(g3treadss,g3tmathss))

# specifying the formula
user_formula <- as.formula(
  "g3tlangss ~ treatment + gender + race + birthmonth + birthyear + SCHLURBN + GRDRANGE + GKENRMNT + GKFRLNCH + GKBUSED + GKWHITE ")


# estimate ITR 
fit <- estimate_itr(
               treatment = treatment,
               form = user_formula,
               data = star_data,
               algorithms = c("causal_forest"),
               budget = 0.2,
               split_ratio = 0.7)


# evaluate ITR 
est <- evaluate_itr(fit)
```

Alternatively, we can train the model with the `caret` package (for further information about `caret`, see [caret](http://topepo.github.io/caret/index.html)). 

```{r caret estimate, message = FALSE, eval = FALSE}
# alternatively (with caret package)

# specify the trainControl method
fitControl <- caret::trainControl(## 3-fold CV
                           method = "repeatedcv",
                           number = 3,
                           ## repeated 3 times
                           repeats = 3)

# specify the tuning grid
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = (1:30)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

# estimate ITR
fit_caret <- estimate_itr(
              treatment = "treatment",
              form = user_formula,
              trControl = fitControl,
              data = star_data,
              algorithms = c("gbm"),
              budget = 0.2,
              split_ratio = 0.7,
              tuneGrid = gbmGrid,
              verbose = FALSE)

# evaluate ITR
est_caret <- evaluate_itr(fit_caret)

# check the final model
est_caret$estimates$models$gbm$finalModel
```

The`summary()` function displays the following summary statistics: (1) population average prescriptive effect `PAPE`; (2) population average prescriptive effect with a budget constraint `PAPEp`; (3) population average prescriptive effect difference with a budget constraint `PAPDp`. This quantity will be computed with more than 2 machine learning algorithms); (4) and area under the prescriptive effect curve `AUPEC`. For more information about these evaluation metrics, please refer to [Imai and Li (2021)](https://arxiv.org/abs/1905.05389); (5) Grouped Average Treatment Effects `GATEs`.  The details of the methods for this design are given in [Imai and Li (2022)](https://arxiv.org/abs/2203.14511).


```{r sp_summary}
# summarize estimates
summary(est)

# similarly for caret
# summary(est_caret)
```

We can extract estimates from the `est` object. The following code shows how to extract the GATE estimates for the writing score with `rlasso` and `lasso` algorithms. 

```{r est_extract, eval = FALSE}
# plot GATE estimates
summary(est)$GATE %>% 
  mutate(group = as_factor(group)) %>%
  ggplot(., aes(
    x = group, y = estimate,
    ymin = lower , ymax = upper, color = algorithm)) +
  ggdist::geom_pointinterval(
    width=0.5,    
    position=position_dodge(0.5),
    interval_size_range = c(0.8, 1.5),
    fatten_point = 2.5) +
  theme_bw() +    
  theme(panel.grid = element_blank(),
        panel.background = element_blank()) +
  labs(x = "Group", y = "GATE estimate") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "#4e4e4e") +
  scale_color_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#076f00", "#0072B2")) 

```  

```{r pic0, echo=FALSE, out.width = '60%'}
knitr::include_graphics("man/figures/gate.png")
```

We plot the estimated Area Under the Prescriptive Effect Curve for the writing score across a range of budget constraints for causal forest.

```{r sp_plot, out.width = '60%'}
# plot the AUPEC 
plot(est)

```

## Example under cross-validation

The package also allows estimate ITR with k-folds cross-validation. Instead of specifying the `split_ratio` argument, we choose the number of folds (`n_folds`). The following code presents an example of estimating ITR with 3 folds cross-validation. In practice, we recommend using 10 folds to get a more stable model performance. 

```{r cv_estimate, message = FALSE, out.width = '60%'}
# estimate ITR 
set.seed(2021)
fit_cv <- estimate_itr(
               treatment = treatment,
               form = user_formula,
               data = star_data,
               trcontrol = fitControl,
               algorithms = c("causal_forest"),
               budget = 0.2,
               n_folds = 3)

# evaluate ITR 
est_cv <- evaluate_itr(fit_cv)

# summarize estimates
summary(est_cv)

# plot the AUPEC 
plot(est_cv)
```


## Example with multiple ML algorithms

We can estimate ITR with various machine learning algorithms and then compare the performance of each model. The package includes all ML algorithms in the  `caret` package and 2 additional algorithms ([causal forest](https://grf-labs.github.io/grf/reference/causal_forest.html) and [bartCause](https://cran.r-project.org/web/packages/bartCause/index.html)).

The package also allows estimate heterogeneous treatment effects on the individual and group-level. On the individual-level, the summary statistics and the AUPEC plot show whether assigning individualized treatment rules may outperform complete random experiment. On the group-level, we specify the number of groups through `ngates` and estimating heterogeneous treatment effects across groups. 


```{r multiple, message = FALSE, warning = FALSE}
# specify the trainControl method
fitControl <- caret::trainControl(
                           method = "repeatedcv",
                           number = 3,
                           repeats = 3)
# estimate ITR
set.seed(2021)
fit_cv <- estimate_itr(
               treatment = "treatment",
               form = user_formula,
               data = star_data,
               trControl = fitControl,
               algorithms = c(
                  "causal_forest", 
                  "bartc",
                  "lasso", # from caret package
                  "rf"), 
               budget = 0.2,
               n_folds = 3)

# evaluate ITR
est_cv <- evaluate_itr(fit_cv)

# summarize estimates
summary(est_cv)
```

We plot the estimated Area Under the Prescriptive Effect Curve for the writing score across different ML algorithms.


```{r multiple_plot, fig.width=8, fig.height=6,fig.align = "center"}
# plot the AUPEC with different ML algorithms
plot(est_cv)
```


For caret models, we can extract the training model and check the model performance. 

```{r caret_model, eval = FALSE}
# extract the caret model
fit$estimates$models$rf %>% 
  ggplot() + theme_bw() 
```

```{r pic2, echo=FALSE, out.width = '60%'}
knitr::include_graphics("man/figures/rf.png")
```